{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1darSp-S10L5uJh_trJFqDHL9kKbEnme0",
      "authorship_tag": "ABX9TyON0L+4847r6ORieky+78EZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElahehJafarigol/Federated-Learning-GANs/blob/main/Minority_based_GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Deep Imbalanced Learning for Weather Data: A Federated Learning Approach**"
      ],
      "metadata": {
        "id": "1e58c1bIO21P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Minority-based GANs"
      ],
      "metadata": {
        "id": "i0bkcG_huXuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implementation"
      ],
      "metadata": {
        "id": "2I9JcS_trlZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet keras==2.9.0\n",
        "!pip install --quiet tensorflow==2.9.2"
      ],
      "metadata": {
        "id": "2q1py7JjP3ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d493a6-205f-4de7-c642-bf814bc8d54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m1.5/1.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.13.0 requires keras<2.14,>=2.13.1, but you have keras 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet imbalanced-learn"
      ],
      "metadata": {
        "id": "obQ8Fi137uwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from  IPython import display\n",
        "import pathlib\n",
        "import shutil\n",
        "import tempfile\n",
        "import warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.filterwarnings('ignore')\n",
        "from pandas.core.common import random_state\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn import preprocessing\n",
        "\n",
        "random_state = 42\n",
        "random_seed = 42\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import datasets, layers, models\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend as K\n",
        "from keras import datasets, layers, models, metrics\n",
        "from keras.layers import GaussianNoise\n",
        "from keras import regularizers\n",
        "\n",
        "from keras.layers.serialization import activation\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers.activation import ReLU\n",
        "from pandas.core.indexes.datetimes import Resolution\n",
        "from keras.layers import Concatenate\n",
        "\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix"
      ],
      "metadata": {
        "id": "oLj8mMEvPO4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Multiple Datasets**"
      ],
      "metadata": {
        "id": "e_AJvempugoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load and preprocess the data"
      ],
      "metadata": {
        "id": "f5NWj1o4uipF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrvpniNfI8l-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "40db796a-090d-4136-9fc8-85bbd59cf4cf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9da1c05d-e259-4bb6-b408-3e7dcca0d5b1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9da1c05d-e259-4bb6-b408-3e7dcca0d5b1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WeatherAUS.csv to WeatherAUS.csv\n",
            "User uploaded file \"WeatherAUS.csv\" with length 14159645 bytes\n"
          ]
        }
      ],
      "source": [
        "# Uploading the data in Google colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the data file as a dataframe\n",
        "import io\n",
        "weather_df = pd.read_csv(io.BytesIO(uploaded['WeatherAUS.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "weather_df.shape\n",
        "\n",
        "Data = weather_df\n",
        "Data.RainToday = [1 if each==\"Yes\" else 0 for each in Data.RainToday]\n",
        "Data.RainTomorrow = [1 if each==\"Yes\" else 0 for each in Data.RainTomorrow]\n",
        "#Data.head()\n",
        "\n",
        "Data = Data.drop(['Sunshine','Evaporation','Cloud3pm','Cloud9am','RISK_MM','Location','Date','WindGustDir',\n",
        "       'WindDir9am', 'WindDir3pm'],axis=1)\n",
        "Data.shape\n",
        "\n",
        "# replace rest of the nulls with respective means\n",
        "fill_feat = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed','WindSpeed9am', 'WindSpeed3pm',\n",
        "             'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm','Temp9am', 'Temp3pm',\n",
        "             'RainToday', 'RainTomorrow']\n",
        "for i in fill_feat:\n",
        "    Data[i].fillna(np.mean(Data[i]),inplace=True)\n",
        "\n",
        "print(Data.shape)\n",
        "Data.dropna(inplace=True)\n",
        "\n",
        "# Separate the features and labels\n",
        "X = Data.drop('RainTomorrow', axis=1)\n",
        "y = Data['RainTomorrow']\n",
        "\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "\n",
        "# Normalize the features\n",
        "scalar = preprocessing.MinMaxScaler(feature_range=(0, 12))\n",
        "norm_data = scalar.fit_transform(X)\n",
        "X = pd.DataFrame(norm_data, columns=[X.columns])\n",
        "X = pd.DataFrame(X.reset_index(drop=True))\n",
        "\n",
        "X = pd.DataFrame(X.reset_index(drop=True))\n",
        "\n",
        "original_indices = set(Data.index)\n",
        "current_indices = set(X.index)\n",
        "\n",
        "missing_indices = original_indices - current_indices\n",
        "print(missing_indices)\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "X_complete = X\n",
        "y_complete = y"
      ],
      "metadata": {
        "id": "LxxJb_aOMJdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde39b0d-d27e-4738-c625-54e826413dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(142193, 14)\n",
            "Counter({0: 110316, 1: 31877})\n",
            "set()\n",
            "X shape: (142193, 13)\n",
            "y shape: (142193,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Data Augmentation"
      ],
      "metadata": {
        "id": "ZsNuphLYiOLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_complete\n",
        "y = y_complete\n",
        "\n",
        "# Get the minority class data\n",
        "minority_class_data = X[y == 1].dropna()\n",
        "minority_class_label = y[y == 1].dropna()"
      ],
      "metadata": {
        "id": "XDw6pGXCvIoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import FALSE\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "latent_dim = 13\n",
        "\n",
        "# Define the generator model\n",
        "def define_generator(latent_dim):\n",
        "    model = Sequential(name=\"Generator_model\")\n",
        "    model.add(Dense(15, activation='relu',\n",
        "                    kernel_initializer='he_uniform',\n",
        "                    input_dim=latent_dim))\n",
        "    model.add(Dense(30, activation='relu'))\n",
        "    model.add(Dense(X.shape[1], activation='linear'))\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\"\"\"def define_generator(latent_dim):\n",
        "    model = Sequential(name=\"Generator_model\")\n",
        "    model.add(Dense(1024, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(X.shape[1], activation='sigmoid'))\n",
        "\n",
        "    # model.summary()\n",
        "    return model\"\"\"\n",
        "\n",
        "# Define the discriminator model\n",
        "def define_discriminator(input_shape):\n",
        "    model = Sequential(name=\"Discriminator_model\")\n",
        "    model.add(Dense(15, input_dim=input_shape,\n",
        "                    activation='relu',\n",
        "                    kernel_initializer='he_uniform'))\n",
        "\n",
        "    model.add(Dense(30, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer= 'adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "\"\"\"def define_discriminator(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=keras.optimizers.SGD(0.0003, 0.5),\n",
        "                  metrics=['accuracy'])\n",
        "    # model.summary()\n",
        "    return model\"\"\"\n",
        "\n",
        "# Train the CGANs model\n",
        "def train(X, y, epochs, batch_size, latent_dim):\n",
        "\n",
        "    # Remove rows with missing data\n",
        "    #X = X.dropna()\n",
        "\n",
        "    # Get the set of valid indices\n",
        "    valid_indices = set(X.index)\n",
        "\n",
        "    # Instantiate the generator and Discriminator\n",
        "    generator = define_generator(latent_dim)\n",
        "    discriminator = define_discriminator(X.shape[1])\n",
        "\n",
        "    # Freeze discriminator weights\n",
        "    \"\"\"One crucial detail in the code below is that we make the Discriminator model non-trainable.\n",
        "     We do this because we want to train the Discriminator separately using a combination of\n",
        "     real and fake (generated) data.\"\"\"\n",
        "    discriminator.trainable = True\n",
        "\n",
        "    # Define the cGANs model\n",
        "    cgan_input = Input(shape=(latent_dim,))\n",
        "    discriminator_output = discriminator(generator(cgan_input))\n",
        "    cgan = Model(cgan_input, discriminator_output)\n",
        "    cgan.compile(loss='binary_crossentropy',\n",
        "                 optimizer=\"adam\")\n",
        "\n",
        "    # Train the CGANs model\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Train the discriminator\n",
        "        \"\"\"The first code snippet is better for training the discriminator in a Wasserstein GAN.\n",
        "        The reason is that in a Wasserstein GAN, the discriminator is trained to optimize\n",
        "        a distance measure between the real and generated data distributions,\n",
        "        while in a traditional GAN, the discriminator is trained to classify real and fake samples.\n",
        "        To achieve this, the discriminator needs to be trained multiple times on both\n",
        "        real and generated data. In the first code snippet, the discriminator is\n",
        "        trained for n_critic iterations on both real and generated data.\n",
        "        This helps the discriminator to better estimate the Wasserstein distance\n",
        "        between the two distributions.\"\"\"\n",
        "\n",
        "        try:\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            generated_data = generator.predict(noise)\n",
        "\n",
        "            # Sample indices from the valid range of indices in the dataset\n",
        "            real_indices = np.random.choice(list(valid_indices), size=batch_size, replace=False)\n",
        "            real_data = X.loc[real_indices]\n",
        "            combined_data = np.concatenate([generated_data, real_data])\n",
        "            labels = np.concatenate([np.zeros((batch_size, 1)), np.ones((batch_size, 1))])\n",
        "            d_loss = discriminator.train_on_batch(combined_data, labels)\n",
        "\n",
        "            # Clip discriminator weights\n",
        "            for layer in discriminator.layers:\n",
        "                weights = layer.get_weights()\n",
        "                weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
        "                layer.set_weights(weights)\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"Skipping row {id} due to KeyError: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Train the generator\n",
        "        discriminator.trainable = False\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        misleading_targets = np.ones((batch_size, 1))\n",
        "        g_loss = cgan.train_on_batch(noise, misleading_targets)\n",
        "\n",
        "        # Print the progress\n",
        "        print(f\"Epoch {epoch} Discriminator Loss: {d_loss} Generator Loss: {g_loss}\")"
      ],
      "metadata": {
        "id": "ukWk5Rn-oLPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 54\n",
        "latent_dim = 13\n",
        "\n",
        "train(minority_class_data, minority_class_label, epochs = epochs, batch_size = batch_size, latent_dim = latent_dim)\n",
        "\n",
        "# Instantiate the generator and Discriminator\n",
        "generator_1 = define_generator(latent_dim)"
      ],
      "metadata": {
        "id": "fJe1TQqmjlkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e73d3f1-ba2b-4e08-c4ec-57ea788f3c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 0 Discriminator Loss: [0.37978479266166687, 0.6574074029922485] Generator Loss: 0.6521952748298645\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch 1 Discriminator Loss: [0.3858913779258728, 0.6203703880310059] Generator Loss: 0.6551533341407776\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 2 Discriminator Loss: [0.3776895999908447, 0.6388888955116272] Generator Loss: 0.6526038646697998\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch 3 Discriminator Loss: [0.3883248567581177, 0.5925925970077515] Generator Loss: 0.6678487062454224\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch 4 Discriminator Loss: [0.3900197744369507, 0.6111111044883728] Generator Loss: 0.6545270085334778\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "Epoch 5 Discriminator Loss: [0.39336416125297546, 0.5740740895271301] Generator Loss: 0.6567858457565308\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch 6 Discriminator Loss: [0.3757130801677704, 0.6851851940155029] Generator Loss: 0.6624701023101807\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 7 Discriminator Loss: [0.3894074857234955, 0.6111111044883728] Generator Loss: 0.6684527397155762\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 8 Discriminator Loss: [0.3709618151187897, 0.7129629850387573] Generator Loss: 0.681700587272644\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 9 Discriminator Loss: [0.3753427267074585, 0.6203703880310059] Generator Loss: 0.6791607141494751\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 10 Discriminator Loss: [0.3733288049697876, 0.6666666865348816] Generator Loss: 0.6686314940452576\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Epoch 11 Discriminator Loss: [0.37251460552215576, 0.6481481194496155] Generator Loss: 0.6846274733543396\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Epoch 12 Discriminator Loss: [0.36751532554626465, 0.7037037014961243] Generator Loss: 0.6790537238121033\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "Epoch 13 Discriminator Loss: [0.37783083319664, 0.6203703880310059] Generator Loss: 0.6805130243301392\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 14 Discriminator Loss: [0.37369304895401, 0.6851851940155029] Generator Loss: 0.6645122170448303\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 15 Discriminator Loss: [0.37645113468170166, 0.6481481194496155] Generator Loss: 0.6876475214958191\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 16 Discriminator Loss: [0.3771354556083679, 0.6203703880310059] Generator Loss: 0.6642901301383972\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 17 Discriminator Loss: [0.3577607572078705, 0.7037037014961243] Generator Loss: 0.6916020512580872\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 18 Discriminator Loss: [0.37271878123283386, 0.6481481194496155] Generator Loss: 0.6617218255996704\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 19 Discriminator Loss: [0.36786919832229614, 0.6481481194496155] Generator Loss: 0.6722611784934998\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 20 Discriminator Loss: [0.3579729497432709, 0.7314814925193787] Generator Loss: 0.6918566226959229\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 21 Discriminator Loss: [0.3669981062412262, 0.6666666865348816] Generator Loss: 0.6703082919120789\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 22 Discriminator Loss: [0.36573490500450134, 0.6666666865348816] Generator Loss: 0.6656956672668457\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 23 Discriminator Loss: [0.36799341440200806, 0.6666666865348816] Generator Loss: 0.6743241548538208\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 24 Discriminator Loss: [0.36840370297431946, 0.6851851940155029] Generator Loss: 0.6855696439743042\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 25 Discriminator Loss: [0.34626808762550354, 0.7685185074806213] Generator Loss: 0.6861447691917419\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 26 Discriminator Loss: [0.3609399199485779, 0.6944444179534912] Generator Loss: 0.6901934742927551\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 27 Discriminator Loss: [0.3622831106185913, 0.6481481194496155] Generator Loss: 0.7015599608421326\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 28 Discriminator Loss: [0.35176563262939453, 0.7222222089767456] Generator Loss: 0.6838229894638062\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 29 Discriminator Loss: [0.3579004108905792, 0.7037037014961243] Generator Loss: 0.6770129203796387\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Epoch 30 Discriminator Loss: [0.35522010922431946, 0.7592592835426331] Generator Loss: 0.6994453072547913\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 31 Discriminator Loss: [0.3526761829853058, 0.7222222089767456] Generator Loss: 0.712417721748352\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 32 Discriminator Loss: [0.3468172252178192, 0.7777777910232544] Generator Loss: 0.6953895092010498\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 33 Discriminator Loss: [0.33981290459632874, 0.8611111044883728] Generator Loss: 0.7104176878929138\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 34 Discriminator Loss: [0.33500543236732483, 0.8518518805503845] Generator Loss: 0.7244945168495178\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Epoch 35 Discriminator Loss: [0.3396245837211609, 0.8611111044883728] Generator Loss: 0.7347701191902161\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 36 Discriminator Loss: [0.33325669169425964, 0.8611111044883728] Generator Loss: 0.7321805357933044\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 37 Discriminator Loss: [0.3297279179096222, 0.9444444179534912] Generator Loss: 0.7485504150390625\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 38 Discriminator Loss: [0.32704874873161316, 0.9166666865348816] Generator Loss: 0.7450644373893738\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 39 Discriminator Loss: [0.3261546492576599, 0.9259259104728699] Generator Loss: 0.7508323192596436\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 40 Discriminator Loss: [0.31558889150619507, 0.9629629850387573] Generator Loss: 0.7837786078453064\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 41 Discriminator Loss: [0.3208180069923401, 0.9074074029922485] Generator Loss: 0.7768527865409851\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 42 Discriminator Loss: [0.3168582320213318, 0.9351851940155029] Generator Loss: 0.7750692963600159\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 43 Discriminator Loss: [0.3167567849159241, 0.9166666865348816] Generator Loss: 0.7905987501144409\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 44 Discriminator Loss: [0.3089653551578522, 0.9444444179534912] Generator Loss: 0.7966247200965881\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 45 Discriminator Loss: [0.30589622259140015, 0.9444444179534912] Generator Loss: 0.8099945783615112\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 46 Discriminator Loss: [0.30389708280563354, 0.9814814925193787] Generator Loss: 0.8258030414581299\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 47 Discriminator Loss: [0.30044519901275635, 0.9722222089767456] Generator Loss: 0.8081605434417725\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 48 Discriminator Loss: [0.29108524322509766, 0.9722222089767456] Generator Loss: 0.827663779258728\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 49 Discriminator Loss: [0.2928037941455841, 0.9907407164573669] Generator Loss: 0.8359259963035583\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 50 Discriminator Loss: [0.29168590903282166, 1.0] Generator Loss: 0.8543137907981873\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 51 Discriminator Loss: [0.2874208092689514, 1.0] Generator Loss: 0.8425983190536499\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 52 Discriminator Loss: [0.281009703874588, 1.0] Generator Loss: 0.8538348078727722\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 53 Discriminator Loss: [0.2858197093009949, 1.0] Generator Loss: 0.866465151309967\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 54 Discriminator Loss: [0.2857268154621124, 1.0] Generator Loss: 0.8478191494941711\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 55 Discriminator Loss: [0.2751173675060272, 1.0] Generator Loss: 0.8815604448318481\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 56 Discriminator Loss: [0.26858383417129517, 1.0] Generator Loss: 0.8647488951683044\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 57 Discriminator Loss: [0.27032753825187683, 1.0] Generator Loss: 0.8965529799461365\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 58 Discriminator Loss: [0.269633948802948, 1.0] Generator Loss: 0.8983849287033081\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 59 Discriminator Loss: [0.27028724551200867, 1.0] Generator Loss: 0.9024560451507568\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 60 Discriminator Loss: [0.2663748860359192, 1.0] Generator Loss: 0.9091475605964661\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 61 Discriminator Loss: [0.2686134874820709, 1.0] Generator Loss: 0.9218106865882874\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 62 Discriminator Loss: [0.26917871832847595, 1.0] Generator Loss: 0.8972609043121338\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 63 Discriminator Loss: [0.263724684715271, 1.0] Generator Loss: 0.9436930418014526\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 64 Discriminator Loss: [0.26104092597961426, 1.0] Generator Loss: 0.9116278290748596\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch 65 Discriminator Loss: [0.26773887872695923, 1.0] Generator Loss: 0.9324498772621155\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch 66 Discriminator Loss: [0.255835622549057, 1.0] Generator Loss: 0.9122022986412048\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 67 Discriminator Loss: [0.262184739112854, 1.0] Generator Loss: 0.919836699962616\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 68 Discriminator Loss: [0.2509390413761139, 1.0] Generator Loss: 0.9332751035690308\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 69 Discriminator Loss: [0.25653064250946045, 1.0] Generator Loss: 0.9476078152656555\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 70 Discriminator Loss: [0.25605902075767517, 1.0] Generator Loss: 0.9549638032913208\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 71 Discriminator Loss: [0.25867724418640137, 1.0] Generator Loss: 0.939397931098938\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 72 Discriminator Loss: [0.253869891166687, 1.0] Generator Loss: 0.9271422028541565\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 73 Discriminator Loss: [0.25430095195770264, 1.0] Generator Loss: 0.9362537264823914\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 74 Discriminator Loss: [0.2547895610332489, 1.0] Generator Loss: 0.9342951774597168\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 75 Discriminator Loss: [0.25646546483039856, 1.0] Generator Loss: 0.9353725910186768\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch 76 Discriminator Loss: [0.25022733211517334, 1.0] Generator Loss: 0.936392068862915\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 77 Discriminator Loss: [0.26045605540275574, 1.0] Generator Loss: 0.9683644771575928\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch 78 Discriminator Loss: [0.24714919924736023, 1.0] Generator Loss: 0.9857930541038513\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 79 Discriminator Loss: [0.2508240044116974, 1.0] Generator Loss: 0.9550623297691345\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 80 Discriminator Loss: [0.2547723650932312, 1.0] Generator Loss: 0.9480708837509155\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 81 Discriminator Loss: [0.24923484027385712, 1.0] Generator Loss: 0.950716495513916\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch 82 Discriminator Loss: [0.2528127431869507, 1.0] Generator Loss: 0.9765648245811462\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 83 Discriminator Loss: [0.2510383427143097, 1.0] Generator Loss: 0.9589240550994873\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 84 Discriminator Loss: [0.25268638134002686, 1.0] Generator Loss: 0.9592845439910889\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 85 Discriminator Loss: [0.2469913512468338, 1.0] Generator Loss: 0.948911726474762\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 86 Discriminator Loss: [0.24710676074028015, 1.0] Generator Loss: 0.9710125923156738\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 87 Discriminator Loss: [0.2615100145339966, 1.0] Generator Loss: 0.9622856378555298\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch 88 Discriminator Loss: [0.25181710720062256, 1.0] Generator Loss: 0.9548762440681458\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch 89 Discriminator Loss: [0.25674426555633545, 1.0] Generator Loss: 0.9483358860015869\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 90 Discriminator Loss: [0.26003214716911316, 0.9907407164573669] Generator Loss: 0.9776464104652405\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 91 Discriminator Loss: [0.2516893744468689, 1.0] Generator Loss: 0.9410552978515625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 92 Discriminator Loss: [0.258578360080719, 1.0] Generator Loss: 0.9641444087028503\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch 93 Discriminator Loss: [0.2618560492992401, 0.9814814925193787] Generator Loss: 0.9270783066749573\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch 94 Discriminator Loss: [0.27520450949668884, 0.9907407164573669] Generator Loss: 0.9185466170310974\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 95 Discriminator Loss: [0.26384174823760986, 1.0] Generator Loss: 0.9286899566650391\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 96 Discriminator Loss: [0.26840540766716003, 0.9814814925193787] Generator Loss: 0.9404787421226501\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 97 Discriminator Loss: [0.2746806740760803, 0.9907407164573669] Generator Loss: 0.8860476613044739\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 98 Discriminator Loss: [0.27139127254486084, 0.9629629850387573] Generator Loss: 0.8987210392951965\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 99 Discriminator Loss: [0.2817308008670807, 1.0] Generator Loss: 0.9025704860687256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
        "\n",
        "# Calculate the class ratio in the balanced dataset\n",
        "class_ratio = sum(y_train == 0) / len(y_train)\n",
        "class_ratio"
      ],
      "metadata": {
        "id": "0jadvsXMt9Dr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef0b6bd-18da-48b8-c44f-9cf0d775ab80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7755155862650984"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the class ratio in the balanced dataset\n",
        "class_ratio = sum(y_test == 0) / len(y_test)\n",
        "class_ratio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL0-JzlN3tCd",
        "outputId": "c0b63b17-48ab-4840-b1e9-87b12d2d166d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7770315411934315"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data using the GAN generator\n",
        "n_samples = len(minority_class_data*3)\n",
        "\n",
        "synthetic_data = generator_1.predict(np.random.normal(0, 1, (n_samples, latent_dim)))\n",
        "\n",
        "# Combine the real and synthetic data to create a balanced dataset\n",
        "X_temp = np.concatenate([minority_class_data, synthetic_data])\n",
        "y_temp = np.concatenate([minority_class_label, np.ones(n_samples)])\n",
        "\n",
        "# Combine the real and synthetic data to create a balanced dataset\n",
        "X_balanced_1 = np.concatenate([X_train, X_temp])\n",
        "y_balanced_1 = np.concatenate([y_train, y_temp])\n",
        "\n",
        "# Calculate the class ratio in the balanced dataset\n",
        "class_ratio = sum(y_balanced_1 == 0) / len(y_balanced_1)\n",
        "print(\"class_ratio:\", class_ratio)\n",
        "\n",
        "print(\"X_balanced_1\", X_balanced_1.shape)\n",
        "print(\"X_train\", X_train.shape)\n",
        "\n",
        "counter = Counter(y_balanced_1)\n",
        "print(counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNB6plt6t6oy",
        "outputId": "de67ce18-38a6-49f6-d806-e5ece70515af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "997/997 [==============================] - 4s 4ms/step\n",
            "class_ratio: 0.4969804177839872\n",
            "X_balanced_1 (177508, 13)\n",
            "X_train (113754, 13)\n",
            "Counter({1.0: 89290, 0.0: 88218})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import seaborn as sns\n",
        "\n",
        "data = X\n",
        "new_data = synthetic_data\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
        "sns.heatmap(data.corr(), annot=True, ax=ax[0], cmap=\"Blues\")\n",
        "sns.heatmap(new_data.corr(), annot=True, ax=ax[1], cmap=\"Blues\")\n",
        "ax[0].set_title(\"Original Data\")\n",
        "ax[1].set_title(\"synthetic Data\")\"\"\""
      ],
      "metadata": {
        "id": "PdeqYrMQ6zXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
        "ax[0].scatter(data.iloc[:, 0], data.iloc[:, 1])\n",
        "ax[1].scatter(new_data.iloc[:, 0], new_data.iloc[:, 1])\n",
        "ax[0].set_title(\"Original Data\")\n",
        "ax[1].set_title(\"synthetic Data\")\"\"\""
      ],
      "metadata": {
        "id": "pTkB0Kbq7DPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = pd.DataFrame(X_balanced_1)\n",
        "df_1['RainTomorrow'] = y_balanced_1\n",
        "\n",
        "# count target class\n",
        "#sn.countplot(x='RainTomorrow', data=df_1,  palette = \"RdBu\")"
      ],
      "metadata": {
        "id": "NjjvePlNwqbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Training Federated Learning"
      ],
      "metadata": {
        "id": "Kazt4SbxxLa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define data shape and number of classes\n",
        "input_shape = (13,)\n",
        "data_shape = (13,)\n",
        "num_classes = 2\n",
        "\n",
        "num_test_samples = len(X_test)\n",
        "num_train_samples = len(X_train)\n",
        "\n",
        "Batch_size = 64\n",
        "communication_round = 10\n",
        "Learning_rate = 0.001\n",
        "Momentum = 0.9\n",
        "Epochs = 30\n"
      ],
      "metadata": {
        "id": "vlA66yGglAM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_clients = 4\n",
        "def create_clients(X_train, y_train, num_clients, initial='client'):\n",
        "    # create a list of client names\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "\n",
        "    # randomize the data before splitting the data between clients\n",
        "    indices = np.random.permutation(len(X_train))\n",
        "    X_shuffled = X_train[indices]\n",
        "    y_shuffled = y_train[indices]\n",
        "\n",
        "    # shard data and place at each client\n",
        "    size = len(X_train) // num_clients\n",
        "    remainder = len(X_train) % num_clients\n",
        "    shards = [(X_shuffled[i:i+size], y_shuffled[i:i+size]) for i in range(0, size*num_clients, size)]\n",
        "    if remainder:\n",
        "        shards[-1] = (np.concatenate([shards[-1][0], X_shuffled[-remainder:]]),\n",
        "                      np.concatenate([shards[-1][1], y_shuffled[-remainder:]]))\n",
        "\n",
        "    # number of clients must equal number of shards\n",
        "    assert(len(shards) == len(client_names))\n",
        "\n",
        "    # create dictionary of clients and their shards\n",
        "    clients = {}\n",
        "    for i in range(len(client_names)):\n",
        "        # create dictionary entry with client name and data shape\n",
        "        clients[client_names[i]] = (shards[i][0].shape, shards[i][1].shape)\n",
        "\n",
        "    return clients\n",
        "\n",
        "def batch_data(data_shard, batchsize=Batch_size):\n",
        "    # unpack data shard into data and labels arrays\n",
        "    X, y = data_shard\n",
        "\n",
        "    # create a tensorflow dataset object from the data and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "    # shuffle and batch the dataset\n",
        "    return dataset.shuffle(len(y)).batch(batchsize)\n",
        "\n",
        "# create clients from training data and associated labels\n",
        "X_train = X_balanced_1\n",
        "y_train = y_balanced_1\n",
        "\n",
        "\n",
        "clients = create_clients(X_train, y_train, num_clients, initial='client')\n",
        "for client in clients:\n",
        "    print(f\"{client}: X={clients[client][0]}, y={clients[client][1]}\")\n",
        "\n",
        "# You can access the shards for each client by indexing the dictionary returned\n",
        "# by create_clients(), like so:\n",
        "client_1_shard = clients['client_1']\n",
        "client_1_shard\n",
        "\n",
        "# create a batched dataset for each client's data shard\n",
        "clients_batched = {}\n",
        "for client in clients:\n",
        "    client_data_shard = (X_train, y_train)\n",
        "    batched_data = batch_data(client_data_shard)\n",
        "    clients_batched[client] = batched_data\n",
        "\n",
        "#process and batch the test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
        "\n",
        "\n",
        "#########################################################################################\n",
        "# Federated Averaging: aggregation method\n",
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the batch_size\n",
        "    batch_size = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*batch_size\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*batch_size\n",
        "    return local_count/global_count\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    # Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "\n",
        "    return avg_grad\n",
        "\n",
        "def test_model(X_test, Y_test, model, communication_round):\n",
        "    cce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    logits = model.predict(X_test, batch_size=Batch_size)\n",
        "    logits = tf.squeeze(logits, axis=1) # remove last dimension from logits\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.round(logits), Y_test)\n",
        "    auc = roc_auc_score(Y_test, logits)\n",
        "    tn, fp, fn, tp = confusion_matrix(Y_test, tf.round(logits)).ravel()\n",
        "    g_mean = np.sqrt(tp/(tp+fn)*tn/(tn+fp))\n",
        "    print('communication_round: {} | global_accuracy: {:.3%} | global_loss: {} | global_AUC: {:.3%} | global_G-mean: {:.3%}'.format(communication_round, acc, loss, auc, g_mean))\n",
        "    return acc, loss, auc, g_mean"
      ],
      "metadata": {
        "id": "eJ2--l2ZzfuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e377066-e785-471e-b513-91e263c0f035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client_1: X=(44377, 13), y=(44377,)\n",
            "client_2: X=(44377, 13), y=(44377,)\n",
            "client_3: X=(44377, 13), y=(44377,)\n",
            "client_4: X=(44377, 13), y=(44377,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = Epochs\n",
        "learning_rate = Learning_rate\n",
        "momentum = Momentum\n",
        "batch_size = Batch_size\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = keras.optimizers.SGD(learning_rate=learning_rate,\n",
        "                                 momentum = momentum,\n",
        "                                 nesterov = False)\n",
        "loss = \"binary_crossentropy\"\n",
        "\n",
        "metrics = [\n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'),\n",
        "      ]\n",
        "\n",
        "\"\"\"class MyModel:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = tf.keras.Sequential([\n",
        "        layers.Dense(256, activation='relu', input_shape=(13,)),\n",
        "        layers.GaussianNoise(stddev = 0.1),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        #layers.GaussianNoise(stddev = 0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        #layers.GaussianNoise(stddev = 0.1),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        return model\"\"\"\n",
        "\n",
        "\"\"\"class MyModel:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = tf.keras.Sequential([\n",
        "        layers.Dense(32, activation='relu', input_shape=(13,)),\n",
        "        layers.GaussianNoise(stddev = 0.1),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(8, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        return model\"\"\"\n",
        "\n",
        "\n",
        "class MyModel:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = tf.keras.Sequential([\n",
        "        layers.Dense(8, activation='relu', input_shape=(13,)),\n",
        "        layers.Dense(8, activation='relu'),\n",
        "        layers.Dense(8, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        return model"
      ],
      "metadata": {
        "id": "KjJGuKT7YdcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smlp_global = MyModel()\n",
        "global_model = smlp_global.build(shape=(13,), classes=2)\n",
        "global_model.summary()\n",
        "\n",
        "global_acc_list = []\n",
        "global_loss_list = []\n",
        "global_auc_list = []\n",
        "global_gmean_list = []\n",
        "\n",
        "# commence global training loop\n",
        "for communication_round in range(communication_round):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    # initial list to collect local model weights after scaling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    # randomize client data - using keys\n",
        "    client_names = list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "\n",
        "    # loop through each client and create new local model\n",
        "    for client in client_names:\n",
        "        smlp_local = MyModel()\n",
        "        local_model = smlp_local.build(shape=(13,), classes=2)\n",
        "        #local_model = smlp_local.build()\n",
        "        local_model.compile(loss=loss,\n",
        "                            optimizer=optimizer,\n",
        "                            metrics=metrics)\n",
        "\n",
        "        # set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        # fit local model with client's data\n",
        "        model_history = local_model.fit(clients_batched[client], epochs=epochs, verbose=1)\n",
        "\n",
        "        # evaluate local model on test data\n",
        "        # test_loss, test_accuracy = local_model.evaluate(X_test, y_test)\n",
        "        # print(f'Local model {client} - test_loss: {test_loss} - test_accuracy: {test_accuracy}')\n",
        "\n",
        "        # scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        # clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    # to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    avg_grad = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    # update global model\n",
        "    global_model.set_weights(avg_grad)\n",
        "\n",
        "    # test global model and print out metrics after each communications round\n",
        "    for X_test, y_test in test_batched:\n",
        "        global_acc, global_loss, global_auc, global_gmean = test_model(X_test, y_test, global_model, communication_round)\n",
        "        global_acc_list.append(global_acc)\n",
        "        global_loss_list.append(global_loss)\n",
        "        global_auc_list.append(global_auc)\n",
        "        global_gmean_list.append(global_gmean)\n"
      ],
      "metadata": {
        "id": "AgS2AdJxDyG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Single Dataset**"
      ],
      "metadata": {
        "id": "HNtiKKWgjIdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load and preprocess the data"
      ],
      "metadata": {
        "id": "I8Go4UR4wPzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading the data in Google colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_1 = files.upload()\n",
        "\n",
        "for fn in uploaded_1.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded_1[fn])))"
      ],
      "metadata": {
        "id": "q6MscpojwV0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the data file as a dataframe\n",
        "import io\n",
        "weather_df = pd.read_csv(io.BytesIO(uploaded_1['Client1.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "weather_df.shape"
      ],
      "metadata": {
        "id": "p80yUSH5wXOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data = weather_df\n",
        "Data.RainToday = [1 if each==\"Yes\" else 0 for each in Data.RainToday]\n",
        "Data.RainTomorrow = [1 if each==\"Yes\" else 0 for each in Data.RainTomorrow]\n",
        "#Data.head()\n",
        "\n",
        "Data = Data.drop(['Sunshine','Evaporation','Cloud3pm','Cloud9am','RISK_MM','Location','Date','WindGustDir',\n",
        "       'WindDir9am', 'WindDir3pm'],axis=1)\n",
        "Data.shape\n",
        "\n",
        "# replace rest of the nulls with respective means\n",
        "fill_feat = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed','WindSpeed9am', 'WindSpeed3pm',\n",
        "             'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm','Temp9am', 'Temp3pm',\n",
        "             'RainToday', 'RainTomorrow']\n",
        "for i in fill_feat:\n",
        "    Data[i].fillna(np.mean(Data[i]),inplace=True)\n",
        "\n",
        "Data.shape\n",
        "Data.dropna(inplace=True)\n",
        "\n",
        "# Separate the features and labels\n",
        "X = Data.drop('RainTomorrow', axis=1)\n",
        "y = Data['RainTomorrow']\n",
        "\n",
        "print(\"X:\", X.shape)\n",
        "print(\"y:\", y.shape)\n",
        "\n",
        "# Normalize the features\n",
        "scalar = preprocessing.MinMaxScaler(feature_range=(0, 12))\n",
        "norm_data = scalar.fit_transform(X)\n",
        "X = pd.DataFrame(norm_data, columns=[X.columns])\n",
        "X = pd.DataFrame(X.reset_index(drop=True))\n",
        "\n",
        "original_indices = set(Data.index)\n",
        "current_indices = set(X.index)\n",
        "\n",
        "missing_indices = original_indices - current_indices\n",
        "print(missing_indices)\n",
        "\n",
        "X_single = X\n",
        "y_single = y"
      ],
      "metadata": {
        "id": "q7ciuRODwanz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Data Augmentation"
      ],
      "metadata": {
        "id": "JOLgl-SMouqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_single\n",
        "y = y_single\n",
        "\n",
        "# Get the minority class data\n",
        "minority_class_data = X[y == 1].dropna()\n",
        "minority_class_label = y[y == 1].dropna()"
      ],
      "metadata": {
        "id": "dBWNqWZXjH3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 64\n",
        "latent_dim = 13\n",
        "\n",
        "train(minority_class_data, minority_class_label, epochs = 20, batch_size = 128, latent_dim = 13)\n",
        "\n",
        "# Instantiate the generator and Discriminator\n",
        "generator_2 = define_generator(latent_dim)"
      ],
      "metadata": {
        "id": "7_PP1kQKl9hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
        "\n",
        "# Calculate the class ratio in the balanced dataset\n",
        "class_ratio = sum(y_train == 0) / len(y_train)\n",
        "class_ratio"
      ],
      "metadata": {
        "id": "khr4yuDPjHz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the generator to create synthetic data\n",
        "synthetic_data = generator_2.predict(np.random.normal(0, 1, (len(X_train), latent_dim)))\n",
        "\n",
        "# Combine the real and synthetic data to create a balanced dataset\n",
        "X_balanced = np.concatenate([X_train, synthetic_data])\n",
        "y_balanced = np.concatenate([y_train, np.zeros(len(X_train))])\n",
        "\n",
        "print(\"X_train\", X_train.shape)\n",
        "print(\"X_balanced\", X_balanced.shape)\n",
        "\n",
        "print(\"X_balanced:\", X_balanced.shape)\n",
        "print(\"y_balanced:\", y_balanced.shape)"
      ],
      "metadata": {
        "id": "ZKiKngKIjHwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Centralized Training"
      ],
      "metadata": {
        "id": "D6eTwaOtkbV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define data shape and number of classes\n",
        "input_shape = (13,)\n",
        "num_classes = 2\n",
        "\n",
        "num_test_samples = len(X_test)\n",
        "num_train_samples = len(X_train)\n",
        "\n",
        "Batch_size = 64\n",
        "communication_round = 10\n",
        "Learning_rate = 0.001\n",
        "Momentum = 0.9\n",
        "Epochs = 30\n",
        "data_shape = (13,)"
      ],
      "metadata": {
        "id": "UCxZjElXjHtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = Epochs\n",
        "learning_rate = Learning_rate\n",
        "momentum = Momentum\n",
        "batch_size = Batch_size\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = keras.optimizers.SGD(learning_rate=learning_rate,\n",
        "                                 momentum = momentum,\n",
        "                                 nesterov = False)\n",
        "loss = \"binary_crossentropy\"\n",
        "\n",
        "metrics = [\n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'),\n",
        "      ]\n",
        "\n",
        "\"\"\"model = tf.keras.Sequential([\n",
        "    layers.Dense(32, activation='relu', input_shape=(13,)),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "    ])\"\"\"\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(8, activation='relu', input_shape=(13,)),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=metrics)\n",
        "\n",
        "model.fit(X_balanced, y_balanced, epochs=Epochs, batch_size=Batch_size)"
      ],
      "metadata": {
        "id": "ulwBwhkYjHm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Evaluation = model.evaluate(X_test, y_test, verbose = 1)"
      ],
      "metadata": {
        "id": "bCI5IGADjHjS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}